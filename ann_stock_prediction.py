# -*- coding: utf-8 -*-
"""ANN Stock Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WfcpagkRp6OeXLi65UYE-8c10N5NkO0S
"""

!pip install yfinance

import yfinance as yf

# Download last 5 years of daily data
df = yf.download('AAPL', start='2019-01-01', end='2024-12-31')

# Save to CSV
df.to_csv('AAPL.csv')

# Preview
print(df.head())

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Load your dataset (replace with your own)
df = pd.read_csv("AAPL.csv")  # Assume it has a "Close" column

# Use only 'Close' prices and ensure it's a numeric array
# Attempt to convert the 'Close' column to numeric, coercing errors to NaN
df['Close'] = pd.to_numeric(df['Close'], errors='coerce')

# Drop rows where 'Close' is NaN (optional, depending on how you want to handle missing data)
df.dropna(subset=['Close'], inplace=True)

data = df['Close'].values.reshape(-1, 1) # Select the 'Close' column and reshape

# --- Add this check to see the content and type of 'data' ---
print("Shape of data before scaling:", data.shape)
print("First few elements of data before scaling:", data[:5])
print("Data type of data before scaling:", data.dtype)
print("Checking for non-finite values in data:", np.any(~np.isfinite(data)))
# ------------------------------------------------------------


# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Create dataset for training (e.g., using last 60 days to predict next day)
def create_dataset(data, look_back=60):
    X, y = [], []
    for i in range(look_back, len(data)):
        # Ensure indexing is correct for 2D array
        X.append(data[i-look_back:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

look_back = 60
X, y = create_dataset(scaled_data, look_back)

train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

model = Sequential([
    Dense(128, activation='relu', input_shape=(look_back,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')

model.summary()

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(X_train, y_train,
                    epochs=100,
                    batch_size=32,
                    validation_split=0.1,
                    callbacks=[early_stop],
                    verbose=1)

model.save(filepath="trainedmodel.keras")

# Predict
y_pred = model.predict(X_test)

# Inverse transform
y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1, 1))
y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))

# RMSE
rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
print("RMSE:", rmse)

plt.figure(figsize=(12, 6))
plt.plot(y_test_inv, label='True Price')
plt.plot(y_pred_inv, label='Predicted Price')
plt.title('Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.grid()
plt.show()

